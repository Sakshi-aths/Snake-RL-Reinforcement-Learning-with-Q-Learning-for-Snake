{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode: 0, Score: 1, Min Progress: 1, Epsilon: 1.00, No Progress: 0\n",
      "Episode: 10, Score: 7, Min Progress: 7, Epsilon: 0.61, No Progress: 0\n",
      "Episode: 20, Score: 4, Min Progress: 7, Epsilon: 0.40, No Progress: 10\n",
      "Episode: 30, Score: 5, Min Progress: 7, Epsilon: 0.27, No Progress: 20\n",
      "Episode: 40, Score: 3, Min Progress: 12, Epsilon: 0.15, No Progress: 4\n",
      "Episode: 50, Score: 8, Min Progress: 12, Epsilon: 0.10, No Progress: 14\n",
      "Episode: 60, Score: 7, Min Progress: 12, Epsilon: 0.10, No Progress: 24\n",
      "Episode: 70, Score: 7, Min Progress: 12, Epsilon: 0.10, No Progress: 34\n",
      "Episode: 80, Score: 0, Min Progress: 12, Epsilon: 0.10, No Progress: 44\n",
      "Forcing exploration! Epsilon increased to 0.19990187928652475\n",
      "Episode: 90, Score: 8, Min Progress: 12, Epsilon: 0.16, No Progress: 4\n",
      "Episode: 100, Score: 4, Min Progress: 12, Epsilon: 0.10, No Progress: 14\n",
      "Episode: 110, Score: 6, Min Progress: 12, Epsilon: 0.10, No Progress: 24\n",
      "Episode: 120, Score: 12, Min Progress: 12, Epsilon: 0.10, No Progress: 34\n",
      "Episode: 130, Score: 8, Min Progress: 12, Epsilon: 0.10, No Progress: 44\n",
      "Forcing exploration! Epsilon increased to 0.1999966548034474\n",
      "Episode: 140, Score: 5, Min Progress: 14, Epsilon: 0.16, No Progress: 3\n",
      "Episode: 150, Score: 5, Min Progress: 16, Epsilon: 0.10, No Progress: 5\n",
      "Episode: 160, Score: 4, Min Progress: 16, Epsilon: 0.10, No Progress: 15\n",
      "Episode: 170, Score: 6, Min Progress: 16, Epsilon: 0.10, No Progress: 25\n",
      "Episode: 180, Score: 9, Min Progress: 16, Epsilon: 0.10, No Progress: 35\n",
      "Episode: 190, Score: 10, Min Progress: 16, Epsilon: 0.10, No Progress: 45\n",
      "Forcing exploration! Epsilon increased to 0.19999142951678014\n",
      "Episode: 200, Score: 8, Min Progress: 16, Epsilon: 0.15, No Progress: 5\n",
      "Episode: 210, Score: 10, Min Progress: 16, Epsilon: 0.10, No Progress: 15\n",
      "Episode: 220, Score: 2, Min Progress: 16, Epsilon: 0.10, No Progress: 25\n",
      "Episode: 230, Score: 1, Min Progress: 16, Epsilon: 0.10, No Progress: 35\n",
      "Episode: 240, Score: 7, Min Progress: 16, Epsilon: 0.10, No Progress: 45\n",
      "Forcing exploration! Epsilon increased to 0.1999862043666334\n",
      "Episode: 250, Score: 10, Min Progress: 16, Epsilon: 0.15, No Progress: 5\n",
      "Episode: 260, Score: 6, Min Progress: 16, Epsilon: 0.10, No Progress: 15\n",
      "Episode: 270, Score: 6, Min Progress: 16, Epsilon: 0.10, No Progress: 25\n",
      "\n",
      "Training interrupted by user\n",
      "\n",
      "Saving trained agent...\n",
      "Agent saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "class SnakeGame:\n",
    "    def __init__(self, width=400, height=400, grid_size=20):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((width, height + 200))\n",
    "        pygame.display.set_caption('Snake RL')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake = [(self.width//(2*self.grid_size))*self.grid_size,\n",
    "                      (self.height//(2*self.grid_size))*self.grid_size]\n",
    "        self.snake_body = []\n",
    "        self.food = self._place_food()\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.score = 0\n",
    "        self.game_over = False\n",
    "        self.steps_without_food = 0\n",
    "        self.max_steps_without_food = 200  # Increased to give more exploration time\n",
    "        self.current_direction_steps = 0  # Track steps in same direction\n",
    "        self.max_direction_steps = 20  # Maximum steps in same direction\n",
    "        return self._get_state()\n",
    "\n",
    "    def _place_food(self):\n",
    "        while True:\n",
    "            food = [random.randint(1, (self.width-2*self.grid_size)//self.grid_size)*self.grid_size,\n",
    "                   random.randint(1, (self.height-2*self.grid_size)//self.grid_size)*self.grid_size]\n",
    "            if food not in self.snake_body and food != self.snake:\n",
    "                return food\n",
    "\n",
    "    def _get_state(self):\n",
    "        head_x, head_y = self.snake\n",
    "        food_x, food_y = self.food\n",
    "\n",
    "        danger_straight = False\n",
    "        danger_right = False\n",
    "        danger_left = False\n",
    "\n",
    "        # Current direction of movement\n",
    "        current_dir_vector = {\n",
    "            'UP': [0, -self.grid_size],\n",
    "            'DOWN': [0, self.grid_size],\n",
    "            'LEFT': [-self.grid_size, 0],\n",
    "            'RIGHT': [self.grid_size, 0]\n",
    "        }[self.direction]\n",
    "\n",
    "        # Check direction and update dangers\n",
    "        if self.direction == 'UP':\n",
    "            danger_straight = (head_y - self.grid_size < 0) or ([head_x, head_y - self.grid_size] in self.snake_body)\n",
    "            danger_right = (head_x + self.grid_size >= self.width) or ([head_x + self.grid_size, head_y] in self.snake_body)\n",
    "            danger_left = (head_x - self.grid_size < 0) or ([head_x - self.grid_size, head_y] in self.snake_body)\n",
    "        elif self.direction == 'DOWN':\n",
    "            danger_straight = (head_y + self.grid_size >= self.height) or ([head_x, head_y + self.grid_size] in self.snake_body)\n",
    "            danger_right = (head_x - self.grid_size < 0) or ([head_x - self.grid_size, head_y] in self.snake_body)\n",
    "            danger_left = (head_x + self.grid_size >= self.width) or ([head_x + self.grid_size, head_y] in self.snake_body)\n",
    "        elif self.direction == 'LEFT':\n",
    "            danger_straight = (head_x - self.grid_size < 0) or ([head_x - self.grid_size, head_y] in self.snake_body)\n",
    "            danger_right = (head_y - self.grid_size < 0) or ([head_x, head_y - self.grid_size] in self.snake_body)\n",
    "            danger_left = (head_y + self.grid_size >= self.height) or ([head_x, head_y + self.grid_size] in self.snake_body)\n",
    "        elif self.direction == 'RIGHT':\n",
    "            danger_straight = (head_x + self.grid_size >= self.width) or ([head_x + self.grid_size, head_y] in self.snake_body)\n",
    "            danger_right = (head_y + self.grid_size >= self.height) or ([head_x, head_y + self.grid_size] in self.snake_body)\n",
    "            danger_left = (head_y - self.grid_size < 0) or ([head_x, head_y - self.grid_size] in self.snake_body)\n",
    "\n",
    "        state = [\n",
    "            danger_straight,\n",
    "            danger_right,\n",
    "            danger_left,\n",
    "            self.direction == 'LEFT',\n",
    "            self.direction == 'RIGHT',\n",
    "            self.direction == 'UP',\n",
    "            self.direction == 'DOWN',\n",
    "            food_x < head_x,\n",
    "            food_x > head_x,\n",
    "            food_y < head_y,\n",
    "            food_y > head_y,\n",
    "            self.current_direction_steps >= self.max_direction_steps  # New state component\n",
    "        ]\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_without_food += 1\n",
    "        reward = 0  # Initialize reward\n",
    "\n",
    "        # Get previous direction\n",
    "        prev_direction = self.direction\n",
    "        \n",
    "        clock_wise = ['RIGHT', 'DOWN', 'LEFT', 'UP']\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if action == 0:  # Continue straight\n",
    "            new_direction = clock_wise[idx]\n",
    "        elif action == 1:  # Turn right\n",
    "            new_direction = clock_wise[(idx + 1) % 4]\n",
    "        else:  # Turn left\n",
    "            new_direction = clock_wise[(idx - 1) % 4]\n",
    "\n",
    "        # Update direction steps counter\n",
    "        if new_direction == prev_direction:\n",
    "            self.current_direction_steps += 1\n",
    "        else:\n",
    "            self.current_direction_steps = 0\n",
    "\n",
    "        self.direction = new_direction\n",
    "\n",
    "        # Calculate initial position and distance\n",
    "        prev_dist = abs(self.snake[0] - self.food[0]) + abs(self.snake[1] - self.food[1])\n",
    "\n",
    "        # Move snake\n",
    "        x, y = self.snake\n",
    "        if self.direction == 'UP':\n",
    "            y -= self.grid_size\n",
    "        elif self.direction == 'DOWN':\n",
    "            y += self.grid_size\n",
    "        elif self.direction == 'LEFT':\n",
    "            x -= self.grid_size\n",
    "        elif self.direction == 'RIGHT':\n",
    "            x += self.grid_size\n",
    "\n",
    "        self.snake_body.insert(0, list(self.snake))\n",
    "\n",
    "        # Calculate new distance to food\n",
    "        new_dist = abs(x - self.food[0]) + abs(y - self.food[1])\n",
    "\n",
    "        # Reward for moving closer/further from food\n",
    "        if new_dist < prev_dist:\n",
    "            reward += 0.5\n",
    "        else:\n",
    "            reward -= 0.2\n",
    "\n",
    "        # Penalty for moving in same direction too long\n",
    "        if self.current_direction_steps >= self.max_direction_steps:\n",
    "            reward -= 0.5\n",
    "\n",
    "        # Check if food is eaten\n",
    "        if x == self.food[0] and y == self.food[1]:\n",
    "            self.score += 1\n",
    "            reward += 10\n",
    "            self.food = self._place_food()\n",
    "            self.steps_without_food = 0\n",
    "            self.current_direction_steps = 0  # Reset direction steps on food\n",
    "        else:\n",
    "            self.snake_body.pop()\n",
    "            \n",
    "            # Small penalty for not eating food\n",
    "            reward -= 0.05\n",
    "            \n",
    "            # Penalty for being near walls\n",
    "            if x <= self.grid_size or x >= self.width - self.grid_size or \\\n",
    "               y <= self.grid_size or y >= self.height - self.grid_size:\n",
    "                reward -= 0.1\n",
    "\n",
    "        # Check for game over conditions\n",
    "        if (x < 0 or x >= self.width or\n",
    "            y < 0 or y >= self.height or\n",
    "            [x, y] in self.snake_body or\n",
    "            self.steps_without_food >= self.max_steps_without_food):\n",
    "            self.game_over = True\n",
    "            reward = -10\n",
    "            return self._get_state(), reward, True\n",
    "\n",
    "        self.snake = [x, y]\n",
    "        return self._get_state(), reward, False\n",
    "\n",
    "    def render(self, stats=None):\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        pygame.draw.rect(self.screen, (50, 50, 50),\n",
    "                        pygame.Rect(0, 0, self.width, self.height))\n",
    "\n",
    "        # Draw snake head\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0),\n",
    "                        pygame.Rect(self.snake[0], self.snake[1],\n",
    "                                  self.grid_size-2, self.grid_size-2))\n",
    "\n",
    "        # Draw snake body\n",
    "        for segment in self.snake_body:\n",
    "            pygame.draw.rect(self.screen, (0, 200, 0),\n",
    "                           pygame.Rect(segment[0], segment[1],\n",
    "                                     self.grid_size-2, self.grid_size-2))\n",
    "\n",
    "        # Draw food\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0),\n",
    "                        pygame.Rect(self.food[0], self.food[1],\n",
    "                                  self.grid_size-2, self.grid_size-2))\n",
    "\n",
    "        # Draw stats\n",
    "        if stats:\n",
    "            episode, epsilon, avg_score = stats\n",
    "            text_color = (255, 255, 255)\n",
    "            texts = [\n",
    "                f'Episode: {episode}',\n",
    "                f'Score: {self.score}',\n",
    "                f'Epsilon: {epsilon:.2f}',\n",
    "                f'Avg Score: {avg_score:.2f}',\n",
    "                f'Steps: {self.current_direction_steps}'  # Added steps counter\n",
    "            ]\n",
    "            for i, text in enumerate(texts):\n",
    "                text_surface = self.font.render(text, True, text_color)\n",
    "                self.screen.blit(text_surface, (10, self.height + 10 + i * 40))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(10)\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.q_table = {}\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1  # Increased minimum exploration\n",
    "        self.epsilon_decay = 0.9995  # Much slower decay\n",
    "        self.alpha = 0.2  # Learning rate\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "        self.memory = deque(maxlen=10000)  # Increased memory size\n",
    "        self.batch_size = 64\n",
    "        self.min_random_moves = 3  # Minimum random moves before considering Q-values\n",
    "\n",
    "    def get_state_key(self, state):\n",
    "        return tuple(state)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Force minimum random moves\n",
    "        if random.random() < 0.2:  # 20% chance of making minimum random moves\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "\n",
    "        state_key = self.get_state_key(state)\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(self.action_size)\n",
    "\n",
    "        # Epsilon-greedy with temperature-based exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            if random.random() < 0.7:  # 70% of exploration will be weighted random\n",
    "                temp = 2.0\n",
    "                probs = np.exp(self.q_table[state_key] / temp)\n",
    "                probs = probs / np.sum(probs)\n",
    "                return np.random.choice(self.action_size, p=probs)\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        \n",
    "        # Add random noise to Q-values to break ties\n",
    "        q_values = self.q_table[state_key] + np.random.uniform(0, 0.1, self.action_size)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state_key = self.get_state_key(state)\n",
    "        next_state_key = self.get_state_key(next_state)\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(self.action_size)\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = np.zeros(self.action_size)\n",
    "\n",
    "        old_value = self.q_table[state_key][action]\n",
    "        next_max = np.max(self.q_table[next_state_key])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_table[state_key][action] = new_value\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state in batch:\n",
    "            self.learn(state, action, reward, next_state)\n",
    "\n",
    "        # Slower epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        plt.style.use('dark_background')\n",
    "        self.scores = []\n",
    "        self.avg_scores = []\n",
    "        self.epsilons = []\n",
    "        self.running_avg_size = 100\n",
    "\n",
    "    def update(self, score, epsilon):\n",
    "        self.scores.append(score)\n",
    "        self.epsilons.append(epsilon)\n",
    "        avg_score = np.mean(self.scores[-self.running_avg_size:]) if len(self.scores) > 0 else 0\n",
    "        self.avg_scores.append(avg_score)\n",
    "        return avg_score\n",
    "\n",
    "    def save_plots(self, episode):\n",
    "        if episode % 100 == 0:\n",
    "            # Plot scores\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(self.scores, label='Score', alpha=0.4)\n",
    "            plt.plot(self.avg_scores, label='Average Score', linewidth=2)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title('Training Progress')\n",
    "            plt.legend()\n",
    "            plt.savefig('training_progress.png')\n",
    "            plt.close()\n",
    "\n",
    "            # Plot epsilon decay\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(self.epsilons, label='Epsilon', linewidth=2)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Epsilon')\n",
    "            plt.title('Exploration Rate Decay')\n",
    "            plt.legend()\n",
    "            plt.savefig('epsilon_decay.png')\n",
    "            plt.close()\n",
    "\n",
    "def train():\n",
    "    env = SnakeGame()\n",
    "    agent = QLearningAgent(state_size=12, action_size=3)  # Updated state size to 12 due to new direction step state\n",
    "    plotter = Plotter()\n",
    "    episodes = 1000\n",
    "    min_score_progress = -1\n",
    "    consecutive_no_progress = 0\n",
    "    max_no_progress = 50  # Episodes without progress before forcing exploration\n",
    "\n",
    "    try:\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            steps_in_episode = 0\n",
    "\n",
    "            # Force exploration if stuck\n",
    "            if consecutive_no_progress >= max_no_progress:\n",
    "                agent.epsilon = min(1.0, agent.epsilon * 2)\n",
    "                consecutive_no_progress = 0\n",
    "                print(f\"Forcing exploration! Epsilon increased to {agent.epsilon}\")\n",
    "\n",
    "            while not env.game_over:\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                \n",
    "                # Store experience and learn from batch\n",
    "                agent.remember(state, action, reward, next_state)\n",
    "                agent.replay()\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps_in_episode += 1\n",
    "\n",
    "                # Force episode end if stuck\n",
    "                if steps_in_episode > 1000:\n",
    "                    break\n",
    "\n",
    "                # Calculate average score for display\n",
    "                avg_score = plotter.update(env.score, agent.epsilon)\n",
    "\n",
    "                # Render game with stats\n",
    "                env.render(stats=(episode, agent.epsilon, avg_score))\n",
    "\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        raise KeyboardInterrupt\n",
    "\n",
    "            # Update progress tracking\n",
    "            if env.score > min_score_progress:\n",
    "                min_score_progress = env.score\n",
    "                consecutive_no_progress = 0\n",
    "            else:\n",
    "                consecutive_no_progress += 1\n",
    "\n",
    "            # Save plots periodically\n",
    "            plotter.save_plots(episode)\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                print(f'Episode: {episode}, Score: {env.score}, Min Progress: {min_score_progress}, '\n",
    "                      f'Epsilon: {agent.epsilon:.2f}, No Progress: {consecutive_no_progress}')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    finally:\n",
    "        # Save the trained agent\n",
    "        print(\"\\nSaving trained agent...\")\n",
    "        with open('snake_agent.pkl', 'wb') as f:\n",
    "            pickle.dump(agent, f)\n",
    "        print(\"Agent saved successfully!\")\n",
    "        pygame.quit()\n",
    "\n",
    "def play_trained_agent():\n",
    "    \"\"\"\n",
    "    Load and watch a trained agent play\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('snake_agent.pkl', 'rb') as f:\n",
    "            agent = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No trained agent found! Please train the agent first.\")\n",
    "        return\n",
    "\n",
    "    env = SnakeGame()\n",
    "    state = env.reset()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            state, _, done = env.step(action)\n",
    "            env.render(stats=(0, 0, env.score))  # Just show the score\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Game Over! Score: {env.score}\")\n",
    "                state = env.reset()\n",
    "                \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    raise KeyboardInterrupt\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nPlayback stopped by user\")\n",
    "    finally:\n",
    "        pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = input(\"Enter 'train' to train new agent or 'play' to watch trained agent: \").lower()\n",
    "    if mode == 'train':\n",
    "        train()\n",
    "    elif mode == 'play':\n",
    "        play_trained_agent()\n",
    "    else:\n",
    "        print(\"Invalid mode! Please enter either 'train' or 'play'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sakshi\\OneDrive\\Desktop\\python\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] --text TEXT --method\n",
      "                             {extractive,abstractive}\n",
      "ipykernel_launcher.py: error: the following arguments are required: --text, --method\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sakshi\\OneDrive\\Desktop\\python\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import argparse\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and preprocesses the input text.\"\"\"\n",
    "    text = text.strip().replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n",
    "# Extractive Summarization Function\n",
    "def extractive_summary(text):\n",
    "    \"\"\"Generates an extractive summary using Hugging Face's pipeline.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Abstractive Summarization Function\n",
    "def abstractive_summary(text):\n",
    "    \"\"\"Generates an abstractive summary using the T5 model.\"\"\"\n",
    "    model_name = \"t5-small\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# CLI Tool\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"AI-Based Text Summarization Tool\")\n",
    "    parser.add_argument(\"--text\", type=str, required=True, help=\"Input text to summarize\")\n",
    "    parser.add_argument(\"--method\", type=str, choices=[\"extractive\", \"abstractive\"], required=True, help=\"Summarization method\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    text = preprocess_text(args.text)\n",
    "\n",
    "    if args.method == \"extractive\":\n",
    "        summary = extractive_summary(text)\n",
    "    elif args.method == \"abstractive\":\n",
    "        summary = abstractive_summary(text)\n",
    "\n",
    "    print(\"\\nOriginal Text:\\n\", text)\n",
    "    print(\"\\nGenerated Summary:\\n\", summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
